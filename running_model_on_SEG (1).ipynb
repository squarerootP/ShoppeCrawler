{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yv2tDziAbqBd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/labeled_comments.csv')"
      ],
      "metadata": {
        "id": "jcr11_-mb4T2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WBbLBN1FskiC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()\n",
        "df = df[df['label']!= 'neutral']\n",
        "df['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "2A0bo5I-b8Z_",
        "outputId": "70da1875-5092-469e-9fb0-cee0bb14eaec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "negative    9168\n",
              "positive    1457\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>9168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>1457</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Specify the directory for NLTK data\n",
        "nltk_data_dir = os.path.join(os.getcwd(), \"nltk_data\")\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(nltk_data_dir, exist_ok=True)\n",
        "\n",
        "# Append the directory containing NLTK data to the search path before downloading\n",
        "nltk.data.path.append(nltk_data_dir)\n",
        "\n",
        "# Download 'punkt' and other necessary resources to the specified directory\n",
        "nltk.download('punkt_tab', download_dir=nltk_data_dir)\n",
        "nltk.download('stopwords', download_dir=nltk_data_dir)\n",
        "nltk.download('wordnet', download_dir=nltk_data_dir)\n",
        "nltk.download('omw-1.4', download_dir=nltk_data_dir)\n",
        "\n",
        "\n",
        "# Assuming your DataFrame is named 'df' and has columns 'comment' and 'label'\n",
        "\n",
        "# 1. Preprocessing with NLTK\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "        return ' '.join(tokens)\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "df['comment'] = df['comment'].apply(preprocess_text)\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['comment'], df['label'], test_size=0.2, random_state=42, stratify=df['label'])\n",
        "\n",
        "# 3. Convert text data to TF-IDF features\n",
        "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kj67-J-YpCw",
        "outputId": "fc6b15db-913f-4e4b-b218-bcf91c5972b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /content/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /content/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /content/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /content/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Assuming you have your data in X_train_tfidf, X_test_tfidf, y_train, y_test\n",
        "\n",
        "# Define a list of models to compare\n",
        "models = [\n",
        "    ('SVM', SVC(kernel='linear', C=1)),\n",
        "    ('Random Forest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n",
        "    ('Logistic Regression', LogisticRegression(max_iter=1000, random_state=42)),\n",
        "    ('Multinomial Naive Bayes', MultinomialNB())\n",
        "]\n",
        "\n",
        "# Train and evaluate each model\n",
        "results = []\n",
        "for name, model in models:\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred, zero_division=0)\n",
        "\n",
        "    results.append((name, accuracy, report))\n",
        "\n",
        "# Print the results\n",
        "for name, accuracy, report in results:\n",
        "    print(f\"Model: {name}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Classification Report:\\n{report}\\n\")"
      ],
      "metadata": {
        "id": "hXpa_r7CmeSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38579db3-8141-4196-a5ff-cc757147a466"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: SVM\n",
            "Accuracy: 0.9501\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.95      1.00      0.97      1834\n",
            "    positive       0.99      0.64      0.78       291\n",
            "\n",
            "    accuracy                           0.95      2125\n",
            "   macro avg       0.97      0.82      0.88      2125\n",
            "weighted avg       0.95      0.95      0.95      2125\n",
            "\n",
            "\n",
            "Model: Random Forest\n",
            "Accuracy: 0.9680\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.97      1.00      0.98      1834\n",
            "    positive       0.99      0.77      0.87       291\n",
            "\n",
            "    accuracy                           0.97      2125\n",
            "   macro avg       0.98      0.89      0.93      2125\n",
            "weighted avg       0.97      0.97      0.97      2125\n",
            "\n",
            "\n",
            "Model: Gradient Boosting\n",
            "Accuracy: 0.9581\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.96      1.00      0.98      1834\n",
            "    positive       0.99      0.70      0.82       291\n",
            "\n",
            "    accuracy                           0.96      2125\n",
            "   macro avg       0.97      0.85      0.90      2125\n",
            "weighted avg       0.96      0.96      0.96      2125\n",
            "\n",
            "\n",
            "Model: Logistic Regression\n",
            "Accuracy: 0.9318\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.93      1.00      0.96      1834\n",
            "    positive       0.99      0.51      0.67       291\n",
            "\n",
            "    accuracy                           0.93      2125\n",
            "   macro avg       0.96      0.75      0.82      2125\n",
            "weighted avg       0.94      0.93      0.92      2125\n",
            "\n",
            "\n",
            "Model: Multinomial Naive Bayes\n",
            "Accuracy: 0.8739\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.87      1.00      0.93      1834\n",
            "    positive       1.00      0.08      0.15       291\n",
            "\n",
            "    accuracy                           0.87      2125\n",
            "   macro avg       0.94      0.54      0.54      2125\n",
            "weighted avg       0.89      0.87      0.82      2125\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}